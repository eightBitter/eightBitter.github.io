---
published: true
title: Testing Assumptions Through Survey
layout: post
---
This semester I've had the privilege of serving on a committee that is looking at ways to improve (unnamed) services. During this time we've discussed many issues and bounced around several strategies. The conversations commonly began with "I think they would do *y*" or "*X* would search for it *y* way". 

We drafted out a strategy and then designed a survey to see if our strategy was sound, in other words, we wanted to test the assumptions we had relied on during the draft. It turns out only about half of our assumptions held true. There were several surprises we discovered through the survey; some good, some... that presented challenges.

Overall, I am glad we did the survey. I struggle with [false-consensus bias](https://en.wikipedia.org/wiki/False-consensus_effect) more than I care to admit. It's a challenge I have in my metadata work as well. "I'm going to aggregate these images under *x* facet, because I think users would use said functionality", although I never tested the assumption in the first place. I feel us metadata people need to venture out into the world, or at least to our reference staff, and really test our assumptions. If we don't truly understand our users and how they search, how can we optimize our descriptive metadata?

This was a good learning experience. I encourage others (and myself) to test your assumptions as much as you can!